---
title: "Import Link2Care Survey Data - QDS Visit 1"
date: "2020-12-23 <br> Updated: `r Sys.Date()`"
---

# ‚≠êÔ∏èOverview

This file is used to import the Link2Care follow-up visit survey data and do some initial data cleaning. 

**NOTE on using multiple import code files:**
Previously, I was trying to clean all the separate data frames (i.e, QDS v1-v5, REDCap, and Master Log) in a single Rmd file. 

In theory, this is more efficient than importing and cleaning each visit file separately. For example, we can clean "race" instead of cleaning "race_v1", "race_v2", etc.

However, there are at least two issues with that approach:   
1. There are intentional differences in the variable names (e.g., "V1" and "V2"), and unintentional errors (e.g., HEIGHT_3) in the variable names, from data frame to data frame that make combining them very difficult.   
2. These differences and errors can be "fixed"; however, doing so makes the variable names so different from the codebook that the codebook is barely usable.

For now, I'm going to try importing and cleaning each data file using a separate code file. There may be opportunities to make this more efficient in the future.

**NOTE on data sources:**
* The QDS data is exported from QDS in SPSS data format. The data has to be exported by visit (i.e., v1, v2, ... v5). The SPSS data is then added to the UTHealth servers.

* Some of the v3-v5 visits are also done using REDCap. That data is exported from REDCap and added to the UTHealth servers.

**NOTE on Kiteworks:**
Currently, I'm importing all of this data from the UTHealth server. The data should also be available on Kiteworks if you are unable to connect to the server for some reason.


# üì¶Load packages

```{r message=FALSE}
library(dplyr)
library(haven)
library(readr)
library(tidyr)
library(stringr)
library(officer)
library(testthat)
```

# üåéConnect to UTH server 

```{bash eval=FALSE}
# Don't drill all the way down to live documents because not all of the data is in live documents.
open 'smb://islgpcifs.uthouston.edu/sph_research/Link2Care/'
```


# üì•Import data 

```{r}
v1 <- read_sav("/Volumes/Link2Care/Participant Data/SPSS Data/QDS Data/Visit_1_Data.SAV")

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "QDS Visit 1 imported with", nrow(v1), "rows and", ncol(v1), "columns.\n"
)

# 2021-04-23: QDS Visit 1 imported with 264 rows and 803 columns.
```


# Remove columns of missing data

There are quite a few columns that have missing values for all rows. We
drop those columns below.

```{r}
v1 <- v1 %>% 
  select(
    where(
      ~ !all(is.na(.x))
    )
  )
```

```{r}
ncol(v1) # 795 So, 803 - 795 = 8 columns dropped.
```


# Clean variable names

_See "Notes on cleaning individual L2C data sets for merging" for the rationale, style guidelines, and instructions for this section._
  
## Extract content from Word codebook

```{r}
v1_cb_content <- read_docx("docs/codebooks/L2C_V1 Codebook.docx") %>% 
  docx_summary()
```

## Keep the variables of interest only

```{r}
v1_cb_col_names <- v1_cb_content %>%
  filter(style_name%in% c("section_name", "cb_col_name", "col_name")) %>% 
  select(doc_index, style_name:text) %>% 
  pivot_wider(
    names_from = "style_name",
    values_from = "text"
  )
```

## Clean section names

```{r}
v1_cb_col_names <- v1_cb_col_names %>%
  mutate(
    # Remove "Sect-number."
    section_name = str_remove(section_name, "Sect-\\d{1,}."),
    # Remove asterisks
    section_name = str_remove_all(section_name, "\\*"),
    # Remove empty spaces
    section_name = str_trim(section_name),
    # Clean " (SQ_7 - SQ_11)" from exclusion criteria
    section_name = str_remove(section_name, " \\(SQ_7 - SQ_11\\)")
  ) %>% 
  # Fill section down across rows
  fill(section_name)
```

## Record the sections that are at visit 1

We will use this for checking to make sure all of the correct questionnaire sections merge later.

```{r}
q_sections <- list(v1 = unique(v1_cb_col_names$section_name))
# q_sections[["v1"]]
```

```{r}
write_rds(q_sections, "data/questionnaire_section.rds")
```

```{r}
rm(q_sections)
```

## Reduce to one row per column

Currently, cb_col_name and col_name are on separate rows. We will spread the different column names vertically across rows so that we can reduce the data frame down to one row per column.

```{r}
v1_cb_col_names <- v1_cb_col_names %>%
  # Spread cb_col_name across rows
  fill(cb_col_name) %>% 
  # Spread col_name across rows within cb_col_name
  group_by(cb_col_name) %>% 
  fill(col_name, .direction = "up") %>% 
  # # For data checking
  # filter(section_name == "Self-Rated Health Questionnaire") %>%
  # ungroup() %>%
  # slice(-1) %>%
  # summarise(
  #   length(unique(cb_col_name)),
  #   length(unique(col_name))
  # )
  group_by(cb_col_name) %>% 
  filter(row_number() == 1) %>% 
  ungroup() %>% 
  # Remove first row
  slice(-1)
```

```{r}
dim(v1_cb_col_names) # 724   4
```

## Standardize new column names

* Remove _V1 and V1 from column names
* Replace spaces with underscores
* Convert to lower case
* Add underscore in-between the abbreviated tool name and question number

```{r}
v1_cb_col_names <- v1_cb_col_names %>%
  mutate(
    # Remove _V1 from column name
    # Remove V1 at end of column name (e.g., DEM1V1)
    # Don't include values for col_name that were changed in the Word
    # document.
    col_name = if_else(
      is.na(col_name),
      str_replace(cb_col_name, "_V1|V1", ""),
      col_name
    ),
    # Replace spaces with underscores
    col_name = str_replace_all(col_name, " ", "_"),
    # Convert to lower case
    col_name = str_to_lower(col_name),
    # Add underscore in-between the abbreviated tool name and question number
    col_name = str_replace(col_name, "([a-z])(\\d)", "\\1_\\2")
  )
```

## Check for duplicate column names

Check to make sure this process didn't accidentally create any duplicate column names

```{r}
test_that("No duplicate col_names were created in the visit 1 data.", {
  check_dup_col_name <- v1_cb_col_names %>% 
    group_by(col_name) %>% 
    filter(max(row_number()) > 1) %>% 
    pull(col_name)
  
  expect_length(check_dup_col_name, 0)
})
```

Save v1_cb_col_names as a key that maps the new names to the old names in case we need that information in the future. Possibly for the codebook.

```{r}
write_csv(v1_cb_col_names, "data/columns.csv")
```

## Differences between codebook columns and data columns

Check to see what differences, if any, exist between the columns in the codebook and the columns in the actual data frame.

```{r}
in_cb_not_df <- setdiff(v1_cb_col_names$cb_col_name, names(v1))
in_cb_not_df
```

There are **7** column names in the codebook that don't appear in the data.   

* TEST_V1 is a logic test in the survey. It doesn't exist in the data now, but it could at some point in the future. Therefore, we will manually add it to the data and set its value to NA. 

* DEM13EV1, DEM13FV1, DEM13HV1 are dummy variables for high annual incomes. They don't exist in the data now, but they could at some point in the future. Therefore, we will manually add them to the data and set their values to NA.   

* DEM14EV1, DEM14FV1, DEMO16G are dummy variables for high monthly incomes. They don't exist in the data now, but they could at some point in the future. Therefore, we will manually add them to the data and set their values to NA.

```{r}
# Future proof this by checking to see if the variables have changed over time.
# If this test fails in the future, come back and make adjustments to how we 
# handle columns as needed.
test_that("The expected 7 columns exist in the codebook, but not the v1 df.", {
  expect_equal(
    in_cb_not_df,
    c("TEST_V1", "DEM13EV1", "DEM13FV1", "DEM13HV1", "DEM14EV1", "DEM14FV1", "DEMO16G")
  )
})
```

```{r}
v1[in_cb_not_df] <- NA
```

Now, check for columns that exist in the data frame, but not the codebook.

```{r}
in_df_not_cb <- setdiff(names(v1), v1_cb_col_names$cb_col_name)
in_df_not_cb
```

The remainder of these variables are calculated variables created by the SPSS export script. We will save them in a separate data frame and deal with them later. 

```{r}
v1_spss_calc_vars <- v1 %>% 
  select(id = SUBJECT, visit = VISIT_V1, all_of(in_df_not_cb))
```

```{r}
write_sav(v1_spss_calc_vars, "data/v1_spss_calc_vars.sav")
```

Remove calculated variables from the v1 data. This makes it easier to merge with the other QDS data frames later.

```{r}
v1 <- v1 %>% select(!all_of(in_df_not_cb))
```

```{r}
dim(v1) # 264 725
```

Clean up

```{r}
rm(v1_spss_calc_vars, in_cb_not_df, in_df_not_cb)
```

## Replace variable names in the full data frame

Put them in the same order used in the codebook.

```{r}
v1 <- v1 %>% 
  select(all_of(v1_cb_col_names$cb_col_name))

names(v1) <- v1_cb_col_names$col_name
```


# üóëClean up

```{r}
rm(v1_cb_col_names, v1_cb_content)
```


# üñ®Print a message for when this file is being sourced

```{r}
cat(
  paste0(Sys.Date(), ":"),
  "QDS Visit 1 cleaned with", nrow(v1), "rows and", ncol(v1), "columns.\n"
)

# 2021-04-23: QDS Visit 1 cleaned with 264 rows and 725 columns.
```


```{r echo=FALSE}
sessionInfo()
```













