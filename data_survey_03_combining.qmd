---
title: "Step 3 - Batch Processing Data"
date: "2022-10-20 <br> Updated: `r Sys.Date()`"
format: pdf
editor: 
  markdown: 
    wrap: sentence
---

# ‚≠êÔ∏èOverview

This file is step 3 in processing the source data from Link2Care (L2C) into a single data set for analysis.

This file involves processing and merging the data into a combined and organized data set, using the Variable Map created in previous steps.

[Notes on cleaning individual L2C data sets for merging](https://github.com/brad-cannell/link2care_public/wiki/Notes-on-cleaning-individual-L2C-data-sets-for-merging)

## Process Steps

1.  Pre-process DDT, Arrest, and Bridge Session Minutes data sets so they are compatible with a long-format Subject-Visit composite key structure

2.  Import all source data sets (QDS, REDCap, Master Log, TLFB, DDT, Arrest, Bridge Sessions) to create the variable map

    -   Select variables for inclusion in the final composite data set

    -   Set initial standardization of the variable names for all desired variables in the composite data set

    -   Establish order for the variables in the composite data set

    -   Extract variable label and variable value labels from source data sets to be included in the composite data set

3.  **Batch processing of data sets into combined data set**

4.  Recreation of calculated variables

    -   Consolidating redundant variables
    
    -   Creation of calculated variables, including labels and addition to the variable map

5.  Post-processing modifications

    -  Finalize desired variable names
    
    -  Flag PHI variables

## Notes

The end goal of our combined data set was to have the data in long format, with each of the five data-collection visits for all subjects represented by a single row (unique combination of Subject ID and Visit). We achieved pre-processing of the DDT, Arrest, and Bridge Data into this form in the Step 1 file: data_survey_01_preprocess.qmd. We achieved initial mapping and standardization of our data in the Step 2 file: data_survey_02_map_variables.qmd.

In this file we set out to:

-   Merge the QDS and REDCap data sets, as they should contain mutually exclusive Subject-Visit rows.

    -   For Subject-Visit rows present in both data sets, we had to inspect to determine which should 'dominate'

-   Merge the TLFB, DDT, Arrest, and Bridge Session data into their matching Subject-Visit rows.

    -   This was achieveable due to the pre-processing completed in Step 1: data_survey_01_preprocess.qmd
    
-   Merge the subject-level rows from the Arrest, Bridge Session, and Master Log data sets based on matching Subject (data appearing for all rows for the subject)

    -   Subject group assignment should be carried over from the Master Log data, overriding any other assignment.

# üì¶Load packages & Functions

```{r, message=FALSE, warning=FALSE}
library(dplyr, warn.conflicts = FALSE)
library(purrr, warn.conflicts = FALSE)
library(haven, warn.conflicts = FALSE)
library(here, warn.conflicts = FALSE)
library(stringr, warn.conflicts = FALSE)
library(lubridate, warn.conflicts = FALSE)
library(readxl, warn.conflicts = FALSE)
library(openxlsx, warn.conflicts = FALSE)
library(readr, warn.conflicts = FALSE)
library(tidyverse, warn.conflicts = FALSE)
```

```{r}
source(here::here("R", "data_cols.R"))
source(here::here("R", "standardize_col_names.R"))
source(here::here("R", "flag_unmatching_variables.R"))
source(here::here("R", "vm_make_source_map.R"))
source(here::here("R", "vm_check.R"))
source(here::here("R", "vm_create_for_instrument.R"))
source(here::here("R", "vm_join_inst_section.R"))
source(here::here("R", "vm_join_sections.R"))
source(here::here("R", "vm_add_variable.R"))
source(here::here("R", "vm_delete_variable.R"))
source(here::here("R", "vm_process_source_df.R"))
source(here::here("R", "vm_process_many_source.R"))
source(here::here("R", "data_mod_check.R"))

rm(visit_window_cols)
rm(avail_col_list)
```

# üì• Import data

## QDS

-   Import all 5 QDS data frames.
-   These data sets were exported directly from QDS as SPSS files (.SAV), and they have already been run through an SPSS syntax file (`spss_syntax/Visit 1.sps`, `spss_syntax/Visit 2.sps`, etc.).
-   The data was exported from QDS by someone other than MBC.
-   The SPSS syntax files were created by someone other than MBC.

```{r}
qds_spss_paths <- paste0(
  here::here("data", "qds"), 
  "/Visit_", 
  1:5, 
  "_Data.SAV"
  )
```

Check the most recent file modification dates and print for user when this file is being sourced.

```{r}
purrr::walk(
  .x = c(1:5),
  .f = function(x) {
    path <- qds_spss_paths[x]
    save_nm <- stringr::str_match(qds_spss_paths[x], ".+/(.+)?$")[,2]
    cat(
      save_nm, "last modified on OneDrive", 
      as.character(file.info(path)$mtime), "\n"
    )
  }
)

# Visit_1_Data.SAV last modified on OneDrive 2023-08-25 10:55:05 
# Visit_2_Data.SAV last modified on OneDrive 2023-08-25 10:55:05 
# Visit_3_Data.SAV last modified on OneDrive 2023-08-25 10:55:05 
# Visit_4_Data.SAV last modified on OneDrive 2023-08-25 10:55:05 
# Visit_5_Data.SAV last modified on OneDrive 2023-08-25 10:55:05
```

We imported our data sets.

```{r}
purrr::walk(
  .x = c(1:5),
  .f = function(x) {
    new_nm <- paste0("v", x)
    path <- here::here(qds_spss_paths[x])
    assign(new_nm, haven::read_sav(path), envir = .GlobalEnv)
  }
)
```


Print a message for when this file is being sourced

```{r}
purrr::walk(
  .x = c(1:5),
  .f = function(x) {
    df_nm <- paste0("v", x)
    df <- get(df_nm, envir = .GlobalEnv)
    # Print a message for when this file is being sourced
    cat(
      paste0(Sys.Date(), ":"),
      df_nm, "imported with", nrow(df), "rows and", ncol(df), "columns.\n"
    )
  }
)

# Data check:
# 2023-12-11: v1 imported with 442 rows and 810 columns.
# 2023-12-11: v2 imported with 406 rows and 213 columns.
# 2023-12-11: v3 imported with 273 rows and 542 columns.
# 2023-12-11: v4 imported with 207 rows and 598 columns.
# 2023-12-11: v5 imported with 178 rows and 649 columns.
```

### Data Check: Changes

We checked for changes to the source data set since this file was last modified. 
We performed our check:
  
```{r}
# Inputs last modified: 2023-12-11

purrr::walk(
  .x = c(1:5),
  .f = function(x) {
    path <- here::here(qds_spss_paths[x])
    df_nm <- paste0("v", x)
    df <- get(df_nm, envir = .GlobalEnv)
    print(paste0(df_nm, ":"))
    print(
      data_mod_check(
        df = df,
        df_path_str = path,
        orig_path_str = paste0(
          here::here("data", "qds"), "/Visit_", 1:5, "_Data.SAV"
          )[x],
        mod_dt = rep('2023-08-25 10:55:05 CDT',5)[x],
        num_cols = c(810, 213, 542, 598, 649)[x],
        num_rows = c(442, 406, 273, 207, 178)[x],
        col_names = qds_col_list[[x]]
        )
      )
  }
)

# [1] "v1: TRUE"
# [1] "v2: TRUE"
# [1] "v3: TRUE"
# [1] "v4: TRUE"
# [1] "v5: TRUE"
```

We purged our paths for memory management.

```{r}
rm(qds_spss_paths)
rm(qds_col_list)
```

## REDCap 

Several subjects completed visits within REDCap, rather than QDS.
We imported the REDCap data set.

```{r}
redcap_spss_path <- here::here("data", "redcap", "All_Visits_Redcap.sav")
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r message=FALSE}
redcap <- haven::read_sav(redcap_spss_path)

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "REDCap imported with", nrow(redcap), "rows and", ncol(redcap), "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "REDCap data last modified on OneDrive", 
      as.character(file.info(redcap_spss_path)$mtime), "\n"
    )

# 2023-12-11: REDCap imported with 119 rows and 845 columns.
# REDCap data last modified on OneDrive 2023-08-25 10:56:25 
```

### Data Check: Changes

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = redcap,
  'df_path_str' = redcap_spss_path,
  'orig_path_str' = here::here("data", "redcap", "All_Visits_Redcap.sav"),
  'mod_dt' = '2023-08-25 10:56:25 CDT',
  'num_rows' = 119,
  'num_cols' = 845,
  'col_names' = redcap_cols
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(redcap_spss_path)
rm(redcap_cols)
```

## Excel Master Log 

We imported the master log data, manually assigning column names and types. 

The Master Log was our only source of several demographic variables, such as subject name, subject date of birth, subject age, and subject care manager. It was also the only source of variables relating to subject continuation vs drop status, or if the research team determined the subject should be omitted from analyses.

The Master Log was also our most reliable source of subject group assignment and demographics such as race, ethnicity, and gender which were also represented on other instruments.

We imported our Master Log with all of these variables.

```{r}
master_log_path  <- here::here("data", "master_log", "master_log.xlsx")
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r}
master_log <- readxl::read_excel(
  master_log_path, 
  sheet = "Screened In",
  col_names = c(
    "id", "baseline_dt",
    "v2_sched", "v2_dt", "v2_late", "v2_noshow",
    "v3_sched", "v3_dt", "v3_late", "v3_noshow",
    "v4_sched", "v4_dt", "v4_late", "v4_noshow",
    "v5_sched", "v5_dt", "v5_late", "v5_noshow", 
    "group", "v2_status", "dropped_status", 
    "name_first", "name_middle", "name_last",
    "gender", "race", "hispanic_latino", "dob", "age", "care_manager", 
    "v1_attend", "v2_attend", "v3_attend", "v4_attend", "v5_attend"
    ),
  col_types = c(
    "text", "skip", "date", "skip",
    rep("date",2), rep("text",2), "skip",
    rep("date",2), rep("text",2), "skip",
    rep("date",2), rep("text",2), "skip",
    rep("date",2), rep("text",2), 
    rep("text", 3),
    rep("text", 3),
    rep("text", 5),
    rep("skip", 5), "text",
    rep("text", 5), rep("skip", 10)
                ),
  skip = 1
) |> 
  # Coerce group to numeric so that it can be combined with the QDS data.
  dplyr::mutate(
    group = dplyr::case_when(
      group == "UCM"    ~ 1,
      group == "UCM+SP" ~ 2,
      group == "L2C"    ~ 3
    )
  ) |>
  # Coerce age to a numeric
  dplyr::mutate(age = as.numeric(age)) |>
  # Remove empty rows
  dplyr::filter(!is.na(id))

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "Master log imported with", nrow(master_log), "rows and", ncol(master_log),
  "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "Master log last modified on OneDrive", 
      as.character(file.info(master_log_path)$mtime), "\n"
    )

# 2023-12-11: Master log imported with 442 rows and 35 columns.
# Master log last modified on OneDrive 2023-09-28 11:52:59
```

#### Data Check: Changes

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = master_log,
  'df_path_str' = master_log_path,
  'orig_path_str' =  here::here("data", "master_log", "master_log.xlsx"),
  'mod_dt' = '2023-09-28 11:52:59 CDT',
  'num_rows' = 442,
  'num_cols' = 35,
  'col_names' = c(
      'id', 'baseline_dt', 'v2_sched', 'v2_dt', 'v2_late', 'v2_noshow', 
      'v3_sched', 'v3_dt', 'v3_late', 'v3_noshow', 'v4_sched', 'v4_dt', 
      'v4_late', 'v4_noshow', 'v5_sched', 'v5_dt', 'v5_late', 'v5_noshow', 
      'group', 'v2_status', 'dropped_status', 'name_first', 'name_middle', 
      'name_last', 'gender', 'race', 'hispanic_latino', 'dob', 'age', 
      'care_manager', 'v1_attend', 'v2_attend', 'v3_attend', 'v4_attend', 
      'v5_attend'
      )
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(master_log_path)
```

## Timeline Follow Back (TLFB)

All Timeline Follow Back (TLFB) section data was stored separately.

```{r}
tlfb_spss_path <- here::here("data", "tlfb", "TLFB_Database.sav")
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r message=FALSE}
tlfb <- haven::read_sav(tlfb_spss_path)

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "TLFB imported with", nrow(tlfb), "rows and", ncol(tlfb), "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "TLFB data last modified on OneDrive", 
      as.character(file.info(tlfb_spss_path)$mtime), "\n"
    )

# 2023-12-11: TLFB imported with 1768 rows and 21 columns.
# TLFB data last modified on OneDrive 2023-08-25 10:56:58 
```

#### Data Check: Changes

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = tlfb,
  'df_path_str' = tlfb_spss_path,
  'orig_path_str' = here::here("data", "tlfb", "TLFB_Database.sav"),
  'mod_dt' = '2023-08-25 10:56:58 CDT',
  'num_rows' = 1768,
  'num_cols' = 21,
  'col_names' = tlfb_cols
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(tlfb_spss_path)
rm(tlfb_cols)
```

## Delay Discount Task (DDT)

All Delayed Discount Task (DDT) section data was initially processed with SPSS. This data was in wide format, rather than a Subject-Visit key compatible format. Modification to achieve this format was achieved in the Step 1 file: data_survey_01_preprocess.qmd

```{r}
ddt_rds_path <- here::here("data", "ddt", "l2c_ddt_database_2_long.rds")
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r message=FALSE}
ddt <- readr::read_rds(ddt_rds_path)

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "Pre-processed DDT imported with", nrow(ddt), "rows and", 
  ncol(ddt), "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "Pre-processed DDT data last modified on OneDrive", 
      as.character(file.info(ddt_rds_path)$mtime), "\n"
    )

# 2023-12-11: Pre-processed DDT imported with 1768 rows and 4 columns.
# Pre-processed DDT data last modified on OneDrive 2023-12-11 12:21:45 
```

#### Data Check: Changes

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = ddt,
  'df_path_str' = ddt_rds_path,
  'orig_path_str' = here::here("data", "ddt", "l2c_ddt_database_2_long.rds"),
  'mod_dt' = '2023-12-11 12:21:45  CDT',
  'num_rows' = 1768,
  'num_cols' = 4,
  'col_names' = ddt_pivot_cols
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(ddt_rds_path)
rm(ddt_orig_cols)
rm(ddt_pivot_cols)
```

## Arrest Data

Arrest Data was original in EXCEL format. This data was in a wide Subject format, rather than Subject-Visit key format, where variables were lists of dates.

Modification into a Subject-Visit key format was achieved in the Step 1 file: data_survey_01_preprocess.qmd

```{r}
arrest_rds_path <- here::here("data", "arrest", "arrest_1.rds")
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r message=FALSE}
arrest_data <- readr::read_rds(arrest_rds_path)

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "Pre-processed Arrest data imported with", nrow(arrest_data), "rows and", 
  ncol(arrest_data), "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "Pre-processed Arrest data last modified on OneDrive", 
      as.character(file.info(arrest_rds_path)$mtime), "\n"
    )

# 2023-12-11: Pre-processed Arrest data imported with 442 rows and 10 columns.
# Pre-processed Arrest data last modified on OneDrive 2023-12-11 12:21:49 
```

#### Data Check: Changes

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = arrest_data,
  'df_path_str' = arrest_rds_path,
  'orig_path_str' = here::here("data", "arrest", "arrest_1.rds"),
  'mod_dt' = '2023-12-11 12:21:49 CDT',
  'num_rows' = 442,
  'num_cols' = 10,
  'col_names' = arrest_proc_cols
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(arrest_rds_path)
rm(arrest_orig_cols)
rm(arrest_proc_cols)
```

## Bridge Session Data

Bridge Session Data was original in EXCEL format. This data was in a wide Subject format, rather than Subject-Visit key format, where variables were lists of dates.

Modification into a Subject-Visit key format was achieved in the Step 1 file: data_survey_01_preprocess.qmd

```{r}
bridge_rds_path <- here::here("data", "bridge_session_data", "bridge_1.rds")
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r message=FALSE}
bridge_data <- readr::read_rds(bridge_rds_path)

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "Pre-processed Bridge data imported with", nrow(bridge_data), "rows and", 
  ncol(bridge_data), "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "Pre-processed Bridge data last modified on OneDrive", 
      as.character(file.info(bridge_rds_path)$mtime), "\n"
    )

# 2023-12-11: Pre-processed Bridge data imported with 334 rows and 61 columns.
# Pre-processed Bridge data last modified on OneDrive 2023-12-11 12:22:01
```

#### Data Check: Changes

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = bridge_data,
  'df_path_str' = bridge_rds_path,
  'orig_path_str' = here::here("data", "bridge_session_data", "bridge_1.rds"),
  'mod_dt' = '2023-12-11 12:22:01 CDT',
  'num_rows' = 334,
  'num_cols' = 61,
  'col_names' = bridge_proc_cols
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(bridge_rds_path)
rm(bridge_orig_cols)
rm(bridge_proc_cols)
```

## Variable Map

We imported our Variable Map.

```{r}
variable_map_path <- here::here(
  "data", "Combined Participant Data", "variable_map_01.rds"
  )
```

Import the data.
Check the most recent file modification dates and print for user when this file is being sourced.

```{r message=FALSE}
variable_map <- readr::read_rds(variable_map_path)

# Print a message for when this file is being sourced
cat(
  paste0(Sys.Date(), ":"),
  "Variable Map imported with", nrow(variable_map), "rows and", 
  ncol(variable_map), "columns.\n"
)

# Check the most recent file modification dates and print for user when this
# file is being sourced.

cat(
      "Variable Map data last modified on OneDrive", 
      as.character(file.info(variable_map_path)$mtime), "\n"
    )

# 2023-12-11: Variable Map imported with 979 rows and 19 columns.
# Variable Map data last modified on OneDrive 2023-12-11 16:15:12 
```

#### Data Check: Changes

We ensured our Variable Map met our required minimum format.

```{r}
vm_check(variable_map)
# TRUE
```

We checked for changes to the source data set since this file was last modified.

```{r}
# Inputs last modified: 2023-12-11

data_mod_check(
  'df' = variable_map,
  'df_path_str' = variable_map_path,
  'orig_path_str' = here::here(
      "data", "Combined Participant Data", "variable_map_01.rds"
      ),
  'mod_dt' = '2023-12-11 16:15:12 CDT',
  'num_rows' = 1020,
  'num_cols' = 19,
  'col_names' = c(
      'variable', 'section', 'sec_ord', 'instrument', 'inst_ord', 'item_ord',
      'qds_v1', 'qds_v2', 'qds_v3', 'qds_v4', 'qds_v5', 'redcap',
      'master_log', 'tlfb', 'ddt', 'arrest', 'bridge', 'attr_label',
      'attr_var_labels'
      )
  )
# TRUE
```

We purged the import path for memory management.

```{r}
rm(variable_map_path)
```

# Initial Source Processing and Checks

## Removal of Labels from the Variable Map Itself

We removed the labels from the Variable Map itself, to avoid issues calling attribute values using "pull()" with pipes.

```{r}
for (i in 1:length(colnames(variable_map))){
    attributes(variable_map[[i]])$label <- NULL
}
```

## Data Sets Initially Stored with Subject-Visit Key (QDS, REDCap, TLFB)

### QDS

We packaged our QDS source data for aggregated cleaning and processing.

```{r}
source_list <- list(
  'qds_v1' = v1, 'qds_v2' =  v2, 'qds_v3' = v3, 'qds_v4' = v4, 'qds_v5' = v5
  )

dropping_vars <- list(
  'qds_v1' = setdiff(colnames(v1), na.omit(variable_map$qds_v1)),
  'qds_v2' = setdiff(colnames(v2), na.omit(variable_map$qds_v2)),
  'qds_v3' = setdiff(colnames(v3), na.omit(variable_map$qds_v3)),
  'qds_v4' = setdiff(colnames(v4), na.omit(variable_map$qds_v4)),
  'qds_v5' = setdiff(colnames(v5), na.omit(variable_map$qds_v5))
  )

qds_packaged <- list(
  'source_df' = source_list,
  'source_df_col' = names(source_list),
  'excluding_vars' = dropping_vars
)
```

We processed our combined QDS Data set using the Variable Map.
We received a warning that there were variables with conflicting value labels.

```{r}
qds_data <- vm_process_many_source(variable_map, qds_packaged)

# Warning: `..1$read_3` and `..2$read_3` have conflicting value labels.
# ‚Ñπ Labels for these values will be taken from `..1$read_3`.
# ‚úñ Values: 0Warning: `..1$read_3` and `..2$read_3` have conflicting value 
# labels.
# ‚Ñπ Labels for these values will be taken from `..1$read_3`.
# ‚úñ Values: 0Warning: `..1$read_3` and `..2$read_3` have conflicting value 
# labels.
# ‚Ñπ Labels for these values will be taken from `..1$read_3`.
# ‚úñ Values: 0Warning: `..1$read_3` and `..2$read_3` have conflicting value 
# labels.
# ‚Ñπ Labels for these values will be taken from `..1$read_3`.
# ‚úñ Values: 0
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(qds_data)){
  attributes(
    qds_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     qds_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

We converted the variable `today`, which stored date-time data as a character, into date-time data. We also converted the `drinks` variable from character to numeric.

```{r}
qds_data <- qds_data |>
  dplyr::mutate(
    today = as.Date(today),
    drinks = as.numeric(drinks)
    )
```

We checked to determine if all rows had a unique combination of Subject ID and Visit.

This check was failed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  qds_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# FALSE
```

On manual inspection, it was found that Subject 2070 had two occurrences of Visit 3 recorded in the QDS data.

One occurrence date matched the Master Log record of Visit 3, while the other matched the Master Log record of Visit 4.

The subject was absent from the REDCap data set.

In comparing the variables present in Visit 4 that were not present in Visit 3, we were able to conclude that the occurrence matching Visit 3's Master Log date represented Visit 3 and the one that matched Visit 4's Master Log was mislabeled and actually represented Visit 4.

```{r}
checking <- qds_data |>
  dplyr::filter(subject == 2070 & visit == 3) |>
  dplyr::mutate(
    baseline_dt = master_log[master_log$id == 2070,]$baseline_dt,
    today = master_log[master_log$id == 2070,]$v3_dt
    ) |>
  dplyr::relocate(subject, visit, baseline_dt, today)

checking <- checking |>
  dplyr::select(subject, visit, today,
         all_of(setdiff(
                  variable_map[!is.na(variable_map$qds_v4),]$variable,
                  variable_map[!is.na(variable_map$qds_v3),]$variable
                  )
                )
         )
```

We applied our correction.

```{r}
qds_data <- qds_data |>
  dplyr::mutate(visit = ifelse(
      (subject == 2070 & today == as.Date('2019-02-14')), 
      4, 
      visit
      )
    )
```

We re-checked to determine if all rows had a unique combination of Subject ID and Visit.
This check was now passed

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  qds_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

We purged the raw QDS data sets and the packaged source data to reduce memory burden and protect against potential memory leak.

```{r}
rm(v1)
rm(v2)
rm(v3)
rm(v4)
rm(v5)
rm(source_list)
rm(qds_packaged)
rm(dropping_vars)
```

### REDCap

We processed our REDCap data set using our variable map.

```{r}
redcap_data <- vm_process_source_df(
  variable_map, redcap, 'redcap', setdiff(
    colnames(redcap), variable_map$redcap
    )
  )
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(redcap_data)){
  attributes(
    redcap_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     redcap_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

The REDCAP data contained testing subjects, which all contained the term 'test' or were otherwise strings.
We coerced 'subject' field to a numeric, converting these strings into empty values.
We then dropped the 'test' rows.

```{r}
redcap_data <- redcap_data |>
  dplyr::mutate(subject = as.numeric(subject)) |>
  dplyr::filter(!is.na(subject))

# Warning: There was 1 warning in `dplyr::mutate()`.
# ‚Ñπ In argument: `subject = as.numeric(subject)`.
# Caused by warning:
# ! NAs introduced by coercion
```

We also converted the variables `today`, `vac_2`, and `vac_3`, which were stored as character data, into date-time data. We converted the empty numeric variables `sq_9l` and `bh_18b1` into character data.

```{r}
redcap_data <- redcap_data |>
  dplyr::mutate(
    today = as.Date(today),
    vac_2 = as.Date(vac_2),
    vac_3 = as.Date(vac_3),
    sq_9l = as.character(sq_9l),
    bh_18b1 = as.character(bh_18b1),
    )
```

The REDCap data had a key variable, `redcap_event_name`, which duplicated `visit`.
Prior to dropping this variable, we checked to see if it was consistent across all visits.
We found that there were issues that required manual verification.

```{r}
# DATA CHECK: ensure visit number in 'redcap_event_name' matches visit number
# in the 'visit' variable

checking <- redcap_data |>
  dplyr::mutate(
    redcap_event_name = stringr::str_replace_all(
      redcap_event_name, '(baseline)', 'visit_1')
    ) |>
  dplyr::mutate(
    rc_visit = as.numeric(stringr::str_match(
      redcap_event_name, '([0-9]+)_arm_([0-9]+)')[,2])
    
  ) |>
  dplyr::mutate(visit_match = ifelse((visit == rc_visit | is.na(visit)), TRUE, FALSE)
         ) |>
  dplyr::filter(!visit_match)

nrow(checking) == 0
# FALSE
```

We manually inspected the QDS and Master Log data for these subjects.

```{r}
checking_subjects <- unique(checking$subject)

checking_qds <- qds_data |>
  dplyr::filter(subject %in% checking_subjects) |>
  dplyr::select(subject, visit, today)

checking_redcap <- redcap_data |>
  dplyr::filter(subject %in% checking_subjects) |>
  dplyr::select(subject, visit, redcap_event_name, today)
  
checking_ml <- master_log |>
  dplyr::filter(id %in% checking_subjects)
```

We found that:

-   **Subject 2403** was listed as visit 2 for 'visit_5_arm_1' in REDCap.

    -   This subject had Visits 1 and 2 in the QDS data

    -   This subject was listed as a no-show for Visits 3 and 4 in the Master Log, with Visits 1, 2, and 5 completed.

    -   The date for the REDCap 'visit_5_arm_1' was closest to the the date originally scheduled for Visit 3 per the Master Log.

    -   The questions answered in the REDCap data were consistent with Visit 5.
        Visit 3 variables contained missing values.

    -   We revised this entry to show it was for **Visit 5**

-   **Subject 2349** was listed as Visit 1 for 'visit_4_arm_1' in REDCap

    -   This subject had Visits 1, 2, and 5 in the QDS data

    -   This subject was listed as a no-show for Visit 3 in the Master Log, with Visits 1, 2, 4 and 5 completed.

    -   The date for the REDCAP 'visit_4_arm_1' was closest to the date originally scheduled for Visit 3 per the Master Log.

    -   The questions answered in REDCAP data were consistent with Visit 4.
        Visit 1 variables contained missing values.

    -   We revised this entry to show it was for **Visit 4**

```{r}
redcap_data <- redcap_data |>
  dplyr::mutate(visit = dplyr::case_when(
          (subject == 2349) & (redcap_event_name == "visit_4_arm_1") ~ 4,
          (subject == 2403) & (redcap_event_name == "visit_5_arm_1") ~ 5,
          TRUE ~ visit
          )
    )
```

The revised REDCap data now contained no discrepancy between `redcap_event_name` and `visit`.

```{r}
# DATA CHECK: ensure visit number in 'redcap_event_name' matches visit number
# in the 'visit' variable

checking <- redcap_data |>
  dplyr::mutate(
    redcap_event_name = stringr::str_replace_all(
      redcap_event_name, '(baseline)', 'visit_1')
    ) |>
  dplyr::mutate(
    rc_visit = as.numeric(stringr::str_match(
      redcap_event_name, '([0-9]+)_arm_([0-9]+)')[,2])
    
  ) |>
  dplyr::mutate(visit_match = ifelse((visit == rc_visit | is.na(visit)), TRUE, FALSE)
         ) |>
  dplyr::filter(!visit_match)

nrow(checking) == 0
# TRUE
```

We checked to determine if all rows had a unique combination of Subject ID and Visit.
The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  redcap_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

We purged the raw REDCap data set to reduce memory burden and protect against potential memory leak.

```{r}
rm(redcap)
```

### Timeline Follow Back (TLFB)

We processed our TLFB set using the Variable Map.

```{r}
tlfb_data <- vm_process_source_df(
  variable_map, tlfb, "tlfb", setdiff(colnames(tlfb), variable_map$tlfb)
  )
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(tlfb_data)){
  attributes(
    tlfb_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
    tlfb_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

We checked to determine if all rows had a unique combination of Subject ID and Visit. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  tlfb_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

The TLFB data had a significant number of missing values represented by the character "." Every column with the exception of `subject` and `visit` required conversion to numeric, where these "." values would be coerced into missing (`NA`) values.

```{r}
tlfb_data <- tlfb_data |>
  dplyr::mutate(across(
    setdiff(colnames(tlfb_data), c('subject', 'visit')), 
    as.numeric))

# Warning: There were 18 warnings in `dplyr::mutate()`.
# The first warning was:
# ‚Ñπ In argument: `across(setdiff(colnames(tlfb_data), c("subject", "visit")),
# as.numeric)`.
# Caused by warning:
# ! NAs introduced by coercion
# ‚Ñπ Run dplyr::last_dplyr_warnings() to see the 17 remaining warnings.
```

We filtered our data set to exclude rows that contained missing values for the 18 primary variables (all variables except `subject`, `visit`, and `days_followed`)

```{r}
tlfb_data <- tlfb_data %>%      # Use |> instead of %>% for code to operate
  dplyr::mutate(na_count = rowSums(
      is.na(select(., -c(subject, visit, days_followed)))
      )
    ) |>
  dplyr::filter(na_count < 18) |>
  dplyr::select(-na_count)
```

We purged the raw TLFB data set to reduce memory burden and protect against potential memory leak.

```{r}
rm(tlfb)
```

### Merging Sets

We compared the QDS and REDCap data to determine if there were any visits that appeared to be recorded in both QDS and REDCap for a subject.

We identified six subjects with a single-visit duplication between the two data sets.

```{r}
# DATA CHECK: Isolating subject-visit keys for any combination of subject and
# visit present in both the QDS and REDCap data sets, which would inhibit a 
# simple merge

qds_keys <- pull(
  qds_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste(subject, visit, sep = "-")) |>
    dplyr::select(key)
  )

redcap_keys <- pull(
  redcap_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste(subject, visit, sep = "-")) |>
    dplyr::select(key)
)

checking_keys <- qds_keys[qds_keys %in% redcap_keys]

checking_keys <- tibble::tibble(
  'id' = stringr::str_match(checking_keys, "([0-9]+)-([0-9]+)")[,2],
  'visit' = stringr::str_match(checking_keys, "([0-9]+)-([0-9]+)")[,3],
  'key' = checking_keys
)
nrow(checking_keys)
# 6
length(unique(checking_keys$id))
# 6
```

In manual inspection, we found that the QDS data should predominate for all collections that appeared in both data sets.

-   For **Subject 2261 Visit 3:**

    -   The date of the REDCap collection had a date prior to the baseline visit in the Master Log.
        The date of the QDS collection matched the date for Visit 3 in the Master Log.

    -   QDS data was both more complete and did not appear to be "pencil whipped" (almost exclusively 0s or non-responses) compared to the REDCap data

    -   The **QDS data** was chosen to dominate for this Subject-Visit row

-   For **Subject 2279 Visit 5:**

    -   The date of the REDCap collection was several months prior to the Visit 5 date recorded in the Master Log.
        The date of the QDS collection matched the date for Visit 5 in the Master Log.

    -   QDS data was both more complete and did not appear to be "pencil whipped" (almost exclusively 0s or non-responses) compared to the REDCap data

    -   The **QDS data** was chosen to dominate for this Subject-Visit row

-   For **Subject 2283 Visit 5:**

    -   The date of the REDCap collection was several months prior to the Visit 5 date recorded in the Master Log.
        The date of the QDS collection matched the date for Visit 5 in the Master Log.

    -   QDS data was both more complete and did not appear to be "pencil whipped" (almost exclusively 0s or non-responses) compared to the REDCap data

    -   The **QDS data** was chosen to dominate for this Subject-Visit row

-   For **Subject 2307 Visit 4:**

    -   The date of the REDCap collection had a date prior to the baseline visit in the Master Log.
        The date of the QDS collection matched the date for Visit 4 in the Master Log.

    -   QDS data was both more complete and did not appear to be "pencil whipped" (almost exclusively 0s or non-responses) compared to the REDCap data

    -   The **QDS data** was chosen to dominate for this Subject-Visit row

-   For **Subject 2314 Visit 3:**

    -   The date of the REDCap collection had a date prior to the baseline visit in the Master Log.
        The date of the QDS collection matched the date for Visit 3 in the Master Log.

    -   QDS data was both more complete and did not appear to be "pencil whipped" (almost exclusively 0s or non-responses) compared to the REDCap data

    -   The **QDS data** was chosen to dominate for this Subject-Visit row

-   For **Subject 2336 Visit 5:**

    -   The date of the REDCap collection had a date one day after the baseline visit in the Master Log.
        The date of the QDS collection matched the date for Visit 5 in the Master Log.

    -   QDS data was both more complete and did not appear to be "pencil whipped" (almost exclusively 0s or non-responses) compared to the REDCap data

    -   The **QDS data** was chosen to dominate for this Subject-Visit row

```{r}
# DATA CHECK: manual review and comparison of QDS and REDCap where the same
# combination of Subject ID and Visit are in both sets.

# Add a source column, create key, and isolate only rows that match
# checking_keys
checking_qds <- qds_data |>
  add_column(source = "qds") |>
  dplyr::mutate(key = paste(subject, visit, sep = "-")) |>
  dplyr::filter(key %in% checking_keys$key) |>
  arrange (subject, visit)

# Add a source column, create key, and isolate only rows that match
# checking_keys
checking_redcap <- redcap_data |>
  add_column(source = "redcap") |>
  dplyr::mutate(key = paste(subject, visit, sep = "-")) |>
  dplyr::filter(key %in% checking_keys$key) |>
  arrange (subject, visit) 


for (i in 1:nrow(checking_keys)){
  bind_rows(
    # Isolate the REDCap rows where the variable appears in both REDCap and
    # the QDS set for the visit
    checking_redcap[i,] |>
      dplyr::select(source, all_of(variable_map[
        (!is.na(variable_map[[paste0(
          "qds_v", unique(checking_redcap[i,]$visit)
          )]]) &
           !is.na(variable_map$redcap)),
                            ]$variable)),
    # Isolate the QDS rows where the variable appears in both REDCap and
    # the QDS set for the visit
    checking_qds[i,] |>
      dplyr::select(source, all_of(variable_map[
        (!is.na(variable_map[[paste0(
          "qds_v", unique(checking_redcap[i,]$visit)
          )]]) &
           !is.na(variable_map$redcap)),
                            ]$variable))
  ) |>
    rowwise() |>
    dplyr::mutate(
    # Add the baseline visit date, and date of the visit collection
    # from the master log
      baseline_dt = master_log[master_log$id == subject,]$baseline_dt,
      ml_visit_date = pull(
        master_log[master_log$id == subject, paste0("v",visit,"_dt")]
        )     
      ) |>
    ungroup() |>
    dplyr::relocate(
      source, subject, group, visit, baseline_dt, today, ml_visit_date
      )
}
```

We dropped the REDCap collections for these 6 visits, in favor of keeping the QDS collection:

-   Subject 2261 Visit 3

-   Subject 2279 Visit 5

-   Subject 2283 Visit 5

-   Subject 2307 Visit 4

-   Subject 2314 Visit 3

-   Subject 2336 Visit 5

```{r}
redcap_data <- redcap_data |>
  dplyr::mutate(key = paste(subject, visit, sep = "-")) |>
  dplyr::filter(!(key %in% checking_keys$key)) |>
  dplyr::select(-key)
```

We performed the initial merge of the QDS, REDCap, and TLFB Data.

```{r}
combined_data <- bind_rows(
  qds_data,
  redcap_data
  ) |>
  full_join(
    tlfb_data,
    by = c('subject', 'visit')
  )
```

We checked to determine if all rows had a unique combination of Subject ID and Visit. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  combined_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

We created a list of Subject-Visit keys present in this data set, for comparison in later steps of processing.

```{r}
valid_keys <- pull(
  combined_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::select(key)
)
```

We purged the QDS, REDCap, and TLFB data sets for memory management.

```{r}
rm(qds_data)
rm(tlfb_data)
rm(checking_ml)
rm(checking_qds)
rm(checking_redcap)
```

## Data which Required Pre-Processing (DDT, Arrest, Bridge)

### Delayed Discount Task

We processed our Delayed Discount Task data set using our variable map. We also converted the variable for visit number into a numeric.

```{r}
ddt_data <- vm_process_source_df(
  variable_map, ddt, 'ddt', setdiff(
    colnames(ddt), variable_map$ddt
    )
  ) |>
  dplyr::mutate(visit = as.numeric(visit))
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(ddt_data)){
  attributes(
    ddt_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     ddt_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

We checked to determine if all rows had a unique combination of Subject ID and Visit. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  ddt_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

We checked to ensure there was no combination of Subject ID and Visit in the DDT data that was not already present in the combined data. This check was failed.

```{r}
## DATA CHECK: Checking if any Subject-Visit keys not otherwise present in
# the combined data exist in this data set

nrow(
  ddt_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::filter(!(key %in% valid_keys))
) == 0
# FALSE
```

On manual inspection, several rows that were missing values for both K and ED50 were present. We eliminated these rows.

```{r}
ddt_data <- ddt_data |>
  dplyr::mutate(drop_flag = ifelse((is.na(ddt_k) & is.na(ddt_ed50)),
                            TRUE,
                            FALSE
                            )
         ) |>
    dplyr::filter(!drop_flag) |>
    dplyr::select(-drop_flag)
```

We rechecked to ensure there was no combination of Subject ID and Visit in the DDT data that was not already present in the combined data. This check was now passed.

```{r}
## DATA CHECK: Checking if any Subject-Visit keys not otherwise present in
# the combined data exist in this data set

nrow(
  ddt_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::filter(!(key %in% valid_keys))
) == 0
# TRUE
```

We merged the DDT data into the combined data set.

```{r}
combined_data <- full_join(
  combined_data,
  ddt_data,
  by = c('subject', 'visit')
  )
```

We checked to determine if all rows had a unique combination of Subject ID and Visit. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  combined_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

We purged the DDT data for memory management.

```{r}
rm(ddt)
rm(ddt_data)
```

### Arrest Data

We processed our Arrest data set using our variable map. 

```{r}
arrest_data <- vm_process_source_df(
  variable_map, arrest_data, 'arrest', setdiff(
    colnames(arrest_data), variable_map$arrest
    )
  )
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(arrest_data)){
  attributes(
    arrest_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     arrest_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

We checked to determine if all rows had a unique Subject ID. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique subject ID

nrow(
  arrest_data |>
    dplyr::select(subject) |>
    dplyr::mutate(
      flag = duplicated(subject)|duplicated(subject, fromLast = TRUE)
      ) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

The arrest data was unique at the subject level, and should appear for each and every row for a subject. As such, we isolated the data into a set of subject-level rows.

There were several variables in the arrest data which we identified as belonging to every row for a subject (subject level data). We isolated these rows.

```{r}
subj_lvl_rows <- arrest_data
```

We purged the arrest data for memory management.

```{r}
rm(arrest_data)
```

### Bridge Session Data

We processed our Bridge Session data set using our variable map.

```{r}
bridge_data <- vm_process_source_df(
  variable_map, bridge_data, 'bridge', setdiff(
    colnames(bridge_data), variable_map$bridge
    )
  )
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(bridge_data)){
  attributes(
    bridge_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     bridge_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

We checked to determine if all rows had a unique combination of Subject ID and Visit. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique subject ID

nrow(
  bridge_data |>
    dplyr::select(subject) |>
    dplyr::mutate(
      flag = duplicated(subject)|duplicated(subject, fromLast = TRUE)
      ) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

The bridge data was subject-level data. We added these columns to our subject-level data set.

```{r}
subj_lvl_rows <- full_join(
  subj_lvl_rows,
  bridge_data,
  by = 'subject'
  )
```


We purged the bridge data for memory management.

```{r}
rm(bridge_data)
```

## Data Values By Subject

### Master Log

We processed our Master Log using our variable map. We also converted the variable for `subject` into a numeric.

```{r}
ml_data <- vm_process_source_df(
  variable_map, master_log, 'master_log', setdiff(
    colnames(master_log), variable_map$master_log
    )
  ) |>
  dplyr::mutate(
    subject = as.numeric (subject)
    )
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(ml_data)){
  attributes(
    ml_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     ml_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

We ensured there were no subjects in the Master Log data that were not already present in the combined data set.

```{r}
# DATA CHECK: ensure there are no subjects in the master log that are not
# already present in the combined data
nrow(ml_data |>
  dplyr::filter(!(subject %in% combined_data$subject))
  ) == 0
# TRUE
```

All rows in the Master Log contained subject-level data. As such, we added the Master Log variables to our subject-level data set.

```{r}
subj_lvl_rows <- full_join(
  subj_lvl_rows,
  ml_data,
  by = 'subject'
)
```

There were only two variables present in the Master Log that were already present in the combined data: `subject` and `group`. We wished to pair based on `subject`, and update all occurrences of `group` to match the Master Log value.

```{r}
colnames(ml_data)[colnames(ml_data) %in% colnames(combined_data)]
```

We joined the Master Log and other subject-level data into the combined data, taking care to update all occurrences of `group` to match the Master Log.

```{r}
combined_data <- combined_data |>
  rows_update(
    subj_lvl_rows |> 
      dplyr::select(subject, group),
    by = 'subject'
    ) |>
  left_join(
    subj_lvl_rows |>
      dplyr::select(-group),
    by = 'subject'
  )
```

We purged the Master Log data for memory management

```{r}
rm(master_log)
rm(ml_data)
rm(subj_lvl_rows)
```


## Data Checks

We checked to determine if all rows had a unique combination of Subject ID and Visit. The check was passed.

```{r}
## DATA CHECK: Checking if all rows have a unique combination of subject ID
## and visit number

nrow(
  combined_data |>
    dplyr::select(subject, visit) |>
    dplyr::mutate(key = paste0(subject,"-", visit)) |>
    dplyr::mutate(flag = duplicated(key)|duplicated(key, fromLast = TRUE)) |>
    dplyr::filter(flag)
) == 0
# TRUE
```

We checked to determine if all subjects had a singular value for group assignment.

```{r}
## DATA CHECK: ensure all subjects have a single group assignment

nrow(
combined_data |>
  dplyr::select(subject, group) |>
  dplyr::distinct() |>
  dplyr::filter(duplicated(subject)|duplicated(subject, fromLast = TRUE))
) == 0
# TRUE
```

We checked to ensure there were no columns in the combined data that were not already present in our variable map

```{r}
## DATA CHECK: ensure no unexpected columns were created

length(setdiff(colnames(combined_data), variable_map$variable)) == 0
# TRUE
```

# Ordering and Labeling

We ordered the variables within our variable map, filtered to only include variables present in our current combined data set, and isolated the variables. We used this ordered list to order the columns in our combined data set.

```{r}
variable_order <- pull(
  variable_map |>
    arrange(sec_ord, inst_ord, item_ord) |>
    dplyr::filter(variable %in% colnames(combined_data)) |>
    dplyr::select(variable)
  )

combined_data <- combined_data[ ,variable_order]
```

We ensured the variables in our processed data set had our desired label and value labels attributes.

```{r}
for (var_name in colnames(combined_data)){
  attributes(
    combined_data[[var_name]]
    )$label <- pull(
      variable_map[variable_map$variable == var_name, 'attr_label']
      )
   attributes(
     combined_data[[var_name]]
     )$labels <- pull(
       variable_map[variable_map$variable == var_name, 'attr_var_labels']
       )[[1]] 
}
```

# Filtering Errant Data

There were four subjects that were identified as having false entries: data for a visit which did not occur. These entries were manually reviewed by the research team and approved for omission by PIs on 2023-11-22.

-   Subject 2319 had an errant entry for Visit 3
-   Subjects 2309, 2317, and 2372 had errant entries for Visit 4

```{r}
combined_data <- combined_data |>
  dplyr::filter( !(subject == 2319 & visit == 3) ) |>
  dplyr::filter( !(subject == 2309 & visit == 4) ) |>
  dplyr::filter( !(subject == 2317 & visit == 4) ) |>
  dplyr::filter( !(subject == 2372 & visit == 4) ) 
```

# üíæ Saving the Processed Data

```{r}
# RDS
combined_data_path <- here::here(
  "data", "Combined Participant Data", "combined_data_01.rds"
  )

readr::write_rds(combined_data, combined_data_path)

# SPSS for MB
combined_data_path <- here::here(
  "data", "Combined Participant Data", "combined_data_01.sav"
  )

haven::write_sav(combined_data, combined_data_path)
```

We purged the remaining variables to reduce memory burden and protect against memory leak.

```{r}
rm(combined_data_path)
rm(combined_data)
rm(redcap_data)
rm(i)
rm(qds_keys)
rm(checking)
rm(checking_subjects)
rm(checking_keys)
rm(valid_keys)
rm(var_name)
rm(variable_order)
rm(redcap_keys)
rm(variable_map)
```
